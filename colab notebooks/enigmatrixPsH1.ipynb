{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShailenderGoyal/Enigmatrix_Salesforce_Hackathon/blob/main/colab%20notebooks/enigmatrixPsH1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wr98fT4lrwp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Force reinstall GPU-compatible PyTorch with Triton support\n",
        "!pip uninstall -y torch torchvision torchaudio numpy\n",
        "\n",
        "# Install GPU-compatible PyTorch\n",
        "!pip install --no-cache-dir --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install all required and compatible packages\n",
        "!pip install --no-cache-dir \\\n",
        "  transformers==4.35.2 \\\n",
        "  sentence-transformers==2.3.1 \\\n",
        "  faiss-cpu==1.7.4 \\\n",
        "  fastapi==0.105.0 \\\n",
        "  uvicorn==0.24.0.post1 \\\n",
        "  python-multipart==0.0.6 \\\n",
        "  pyngrok==7.0.0 \\\n",
        "  langchain==0.0.350 \\\n",
        "  langchain-community==0.0.13 \\\n",
        "  pillow==10.0.1 \\\n",
        "  numpy==1.26.4 \\\n",
        "  psutil\n",
        "\n",
        "#  Restart the runtime after running this cell to apply changes\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"\"  # Replace this with your own token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "id": "t4spXWx0l2en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade google-generativeai\n"
      ],
      "metadata": {
        "id": "X2IwrP9ZKImc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document as LangchainDocument\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import importlib\n",
        "import datetime\n",
        "\n",
        "# add you gemini api key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "class RAGGeminiSystem:\n",
        "    def __init__(self):\n",
        "        self.embedding_model = None\n",
        "        self.vector_store = None\n",
        "\n",
        "        self.embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "        self._setup_gemini_api()\n",
        "        print(\"RAG-Gemini system initialized.\")\n",
        "\n",
        "    def _setup_gemini_api(self):\n",
        "        try:\n",
        "            api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "            if not api_key:\n",
        "                raise ValueError(\"Missing GOOGLE_API_KEY environment variable.\")\n",
        "            genai.configure(api_key=api_key)\n",
        "\n",
        "            self.gemini_model = genai.GenerativeModel(\"models/gemini-2.0-flash-lite\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Gemini API initialization failed: {str(e)}\")\n",
        "\n",
        "    def _load_embedding_model(self):\n",
        "        if self.embedding_model is None:\n",
        "            print(\"ðŸ”„ Loading embedding model...\")\n",
        "            try:\n",
        "                importlib.invalidate_caches()\n",
        "                self.embedding_model = HuggingFaceEmbeddings(\n",
        "                    model_name=self.embedding_model_name\n",
        "                )\n",
        "                print(\"Embedding model loaded.\")\n",
        "            except ImportError:\n",
        "                raise RuntimeError(\n",
        "                    \"sentence-transformers is not installed. Run: pip install sentence-transformers\"\n",
        "                )\n",
        "\n",
        "    def add_documents(self, documents):\n",
        "        try:\n",
        "            self._load_embedding_model()\n",
        "\n",
        "            all_chunks = []\n",
        "            for doc in documents:\n",
        "                chunks = self.text_splitter.split_text(doc[\"content\"])\n",
        "                for chunk in chunks:\n",
        "                    all_chunks.append(\n",
        "                        LangchainDocument(page_content=chunk, metadata=doc[\"metadata\"])\n",
        "                    )\n",
        "\n",
        "            if not all_chunks:\n",
        "                return {\"status\": \"error\", \"message\": \"No valid document content to index.\"}\n",
        "\n",
        "            if self.vector_store is None:\n",
        "                self.vector_store = FAISS.from_documents(all_chunks, self.embedding_model)\n",
        "            else:\n",
        "                self.vector_store.add_documents(all_chunks)\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"message\": f\"Added {len(documents)} documents with {len(all_chunks)} total chunks.\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"status\": \"error\", \"message\": f\"Failed to add documents: {str(e)}\"}\n",
        "\n",
        "    def answer_question(self, question, top_k=3, store_response=True):\n",
        "        try:\n",
        "            if self.vector_store is None:\n",
        "                return {\"status\": \"error\", \"message\": \"Knowledge base is empty.\"}\n",
        "\n",
        "            self._load_embedding_model()\n",
        "\n",
        "            docs = self.vector_store.similarity_search(question, k=top_k)\n",
        "            contexts = [doc.page_content for doc in docs]\n",
        "            combined_context = \"\\n\\n\".join(contexts)\n",
        "\n",
        "            prompt = (\n",
        "                f\"You are a helpful assistant with access to the following context:\\n\\n\"\n",
        "                f\"{combined_context}\\n\\n\"\n",
        "                f\"Based on the above information, answer the following question and respond cleanly without any markup symbols:\\n\"\n",
        "                f\"{question}\"\n",
        "            )\n",
        "\n",
        "            # Use Gemini to generate response\n",
        "            response = self.gemini_model.generate_content(prompt)\n",
        "\n",
        "            if not hasattr(response, \"text\") or not response.text.strip():\n",
        "                answer = \"I don't have enough information to answer that question.\"\n",
        "            else:\n",
        "                answer = response.text.strip()\n",
        "\n",
        "            # Optionally add Gemini's answer to vector DB\n",
        "            if store_response and answer:\n",
        "                metadata = {\n",
        "                    \"source\": \"gemini_response\",\n",
        "                    \"question\": question,\n",
        "                    \"timestamp\": datetime.datetime.now().isoformat()\n",
        "                }\n",
        "                doc = {\"content\": answer, \"metadata\": metadata}\n",
        "                self.add_documents([doc])\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"answer\": answer,\n",
        "                \"sources\": [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in docs]\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"status\": \"error\", \"message\": f\"Failed to answer question: {str(e)}\"}\n"
      ],
      "metadata": {
        "id": "MrtRDFi8Vn5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name=\"philschmid/bart-large-cnn-samsum\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        print(f\"Summarizer initialized with model '{self.model_name}' - model will be loaded when needed\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        if self.tokenizer is None or self.model is None:\n",
        "            print(\"Loading summarization model...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n",
        "            if torch.cuda.is_available():\n",
        "                self.model = self.model.to(\"cuda\")\n",
        "            print(\"Model loaded\")\n",
        "\n",
        "    def _unload_model(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def _smart_chunk(self, text, chunk_size=512, chunk_overlap=50):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \"],\n",
        "        )\n",
        "        return splitter.split_text(text)\n",
        "\n",
        "    def summarize(self, text, max_length=150, min_length=40):\n",
        "        try:\n",
        "            self._load_model()\n",
        "\n",
        "            chunks = self._smart_chunk(text)\n",
        "            summaries = []\n",
        "\n",
        "            for chunk in chunks:\n",
        "                inputs = self.tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "                summary_ids = self.model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    max_length=max_length,\n",
        "                    min_length=min_length,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "                summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "                summaries.append(summary)\n",
        "\n",
        "            final_summary = \" \".join(summaries)\n",
        "\n",
        "            self._unload_model()\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"summary\": final_summary,\n",
        "                \"note\": f\"Processed in {len(chunks)} chunk(s) using LangChain chunking\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self._unload_model()\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": str(e)\n",
        "            }\n"
      ],
      "metadata": {
        "id": "e3jZqQLjV5XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n"
      ],
      "metadata": {
        "id": "qRErFOqlWAel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document as LangchainDocument\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import importlib\n",
        "\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embedding_model = None\n",
        "        self.tokenizer = None\n",
        "        self.qa_model = None\n",
        "        self.vector_store = None\n",
        "\n",
        "        self.embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        self.qa_model_name = \"distilbert-base-cased-distilled-squad\"\n",
        "\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        print(\"RAG system initialized - models will be loaded when needed\")\n",
        "\n",
        "    def _load_embedding_model(self):\n",
        "        if self.embedding_model is None:\n",
        "            print(\"Loading embedding model...\")\n",
        "            try:\n",
        "                importlib.invalidate_caches()\n",
        "                self.embedding_model = HuggingFaceEmbeddings(\n",
        "                    model_name=self.embedding_model_name,\n",
        "                    model_kwargs={\"device\": self.device}\n",
        "                )\n",
        "            except ImportError:\n",
        "                raise RuntimeError(\n",
        "                    \"sentence-transformers is not installed. \"\n",
        "                    \"Install it using: pip install sentence-transformers\"\n",
        "                )\n",
        "            print(\"Embedding model loaded\")\n",
        "\n",
        "    def _load_qa_model(self):\n",
        "        if self.qa_model is None or self.tokenizer is None:\n",
        "            print(\"Loading QA model...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.qa_model_name)\n",
        "            self.qa_model = AutoModelForQuestionAnswering.from_pretrained(self.qa_model_name)\n",
        "            self.qa_model.to(self.device)\n",
        "            print(\"QA model loaded\")\n",
        "\n",
        "    def _unload_qa_model(self):\n",
        "        self.qa_model = None\n",
        "        self.tokenizer = None\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def add_documents(self, documents):\n",
        "        try:\n",
        "            self._load_embedding_model()\n",
        "\n",
        "            # Flatten and chunk text\n",
        "            all_chunks = []\n",
        "            for doc in documents:\n",
        "                chunks = self.text_splitter.split_text(doc[\"content\"])\n",
        "                for chunk in chunks:\n",
        "                    all_chunks.append(\n",
        "                        LangchainDocument(page_content=chunk, metadata=doc[\"metadata\"])\n",
        "                    )\n",
        "\n",
        "            if not all_chunks:\n",
        "                return {\"status\": \"error\", \"message\": \"No valid document content to index.\"}\n",
        "\n",
        "            if self.vector_store is None:\n",
        "                self.vector_store = FAISS.from_documents(all_chunks, self.embedding_model)\n",
        "            else:\n",
        "                self.vector_store.add_documents(all_chunks)\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"message\": f\"Added {len(documents)} documents with {len(all_chunks)} total chunks.\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": f\"Failed to add documents: {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def answer_question(self, question, top_k=3):\n",
        "        try:\n",
        "            if self.vector_store is None:\n",
        "                return {\"status\": \"error\", \"message\": \"Knowledge base is empty.\"}\n",
        "\n",
        "            self._load_embedding_model()\n",
        "\n",
        "            docs = self.vector_store.similarity_search(question, k=top_k)\n",
        "            contexts = [doc.page_content for doc in docs]\n",
        "            combined_context = \" \".join(contexts)\n",
        "\n",
        "            self._load_qa_model()\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                question,\n",
        "                combined_context,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding=True\n",
        "            )\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.qa_model(**inputs)\n",
        "\n",
        "            answer_start = torch.argmax(outputs.start_logits)\n",
        "            answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "            input_ids = inputs[\"input_ids\"][0]\n",
        "            answer = self.tokenizer.convert_tokens_to_string(\n",
        "                self.tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
        "            )\n",
        "\n",
        "            if not answer.strip():\n",
        "                answer = \"I don't have enough information to answer that question.\"\n",
        "\n",
        "            result = {\n",
        "                \"status\": \"success\",\n",
        "                \"answer\": answer,\n",
        "                \"sources\": [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in docs]\n",
        "            }\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "        finally:\n",
        "            self._unload_qa_model()\n"
      ],
      "metadata": {
        "id": "xD87hzJyWFQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentence_transformers\n",
        "print(\"sentence-transformers version:\", sentence_transformers.__version__)\n"
      ],
      "metadata": {
        "id": "JcPT3B8F0JNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Body\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any, Optional\n",
        "import uvicorn\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "app = FastAPI(title=\"Personalized Learning Assistant API\")\n",
        "\n",
        "# Enable CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Initialize our modules\n",
        "\n",
        "summarizer = None\n",
        "rag_system = None\n",
        "gemini_rag_system = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    # Force install inside FastAPI process (very important!)\n",
        "    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\"])\n",
        "    global summarizer, rag_system, gemini_rag_system\n",
        "\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "    rag_system = RAGSystem()\n",
        "    gemini_rag_system = RAGGeminiSystem()\n",
        "    print(\"API initialized - models will be loaded on demand\")\n",
        "\n",
        "# Define request models\n",
        "class Document(BaseModel):\n",
        "    content: str\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "class SummarizeRequest(BaseModel):\n",
        "    text: str\n",
        "    max_length: Optional[int] = 150\n",
        "    min_length: Optional[int] = 40\n",
        "\n",
        "# Define API endpoints\n",
        "@app.post(\"/rag/add_documents\")\n",
        "async def add_documents(documents: List[Document]):\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    plain_docs = [doc.dict() for doc in documents]\n",
        "    result = rag_system.add_documents(plain_docs)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/rag/answer\")\n",
        "async def answer_question(request: QuestionRequest):\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = rag_system.answer_question(request.question)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/gemini_rag/add_documents\")\n",
        "async def add_documents_gemini(documents: List[Document]):\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    plain_docs = [doc.dict() for doc in documents]\n",
        "    result = gemini_rag_system.add_documents(plain_docs)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/gemini_rag/answer\")\n",
        "async def answer_question_gemini(request: QuestionRequest):\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = gemini_rag_system.answer_question(request.question)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/summarize\")\n",
        "async def summarize_text(request: SummarizeRequest):\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = summarizer.summarize(\n",
        "        request.text,\n",
        "        max_length=request.max_length,\n",
        "        min_length=request.min_length\n",
        "    )\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "0yIK7WroWJ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "gUpYDhpGWM3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Start the FastAPI server with ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "# Create a public URL\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start the FastAPI server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n"
      ],
      "metadata": {
        "id": "EpwD36EzWQ6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqQ-XMDuWVuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}